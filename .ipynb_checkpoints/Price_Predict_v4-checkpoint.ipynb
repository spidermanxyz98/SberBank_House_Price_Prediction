{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import operator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "random_state=7\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training data is (30471, 292)\n",
      "The shape of test data is (7662, 291)\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('input/train.csv', parse_dates=['timestamp'])\n",
    "test=pd.read_csv('input/test.csv', parse_dates=['timestamp'])\n",
    "macro=pd.read_csv('input/macro.csv', parse_dates=['timestamp'])\n",
    "\n",
    "test_id=test['id']\n",
    "\n",
    "print('The shape of training data is', train.shape)\n",
    "print('The shape of test data is', test.shape)\n",
    "#print('The shape of macro data is', macro.shape)\n",
    "\n",
    "\n",
    "#fts contains the feature names (exclude id and year)\n",
    "fts=list(train.columns[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the investment type (17693, 292)\n",
      "num of ind_1m 930\n",
      "num of ind_2m 680\n",
      "num of ind_3m 292\n"
     ]
    }
   ],
   "source": [
    "trainsub=train[train.timestamp<'2015-01-01']\n",
    "trainsub=trainsub[trainsub.product_type=='Investment']\n",
    "print('shape of the investment type', trainsub.shape)\n",
    "\n",
    "ind_1m=trainsub[trainsub.price_doc <= 1000000].index\n",
    "ind_2m=trainsub[trainsub.price_doc == 2000000].index\n",
    "ind_3m=trainsub[trainsub.price_doc==3000000].index\n",
    "\n",
    "print('num of ind_1m', len(ind_1m))\n",
    "print('num of ind_2m', len(ind_2m))\n",
    "print('num of ind_3m', len(ind_3m))\n",
    "\n",
    "train_index=set(train.index.copy())\n",
    "\n",
    "for ind, gap in zip([ind_1m, ind_2m, ind_3m], [10, 3, 2]):\n",
    "    ind_set=set(ind)\n",
    "    ind_set_cut=ind.difference(set(ind[::gap]))\n",
    "    \n",
    "    train_index=train_index.difference(ind_set_cut)\n",
    "    \n",
    "train=train.loc[train_index]\n",
    "\n",
    "target=np.log(train.price_doc+1)\n",
    "#number of training and test example\n",
    "n_train=train.shape[0]\n",
    "n_test=test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train 29035\n",
      "num of test 7662\n",
      "num of target 29035\n"
     ]
    }
   ],
   "source": [
    "print('num of train', n_train)\n",
    "print('num of test', n_test)\n",
    "print('num of target', len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'target_transfer' (Series)\n"
     ]
    }
   ],
   "source": [
    "target_transfer=train.price_doc\n",
    "%store target_transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine train and test and Change Categorical Data to Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train plus test (36697, 290)\n"
     ]
    }
   ],
   "source": [
    "#concatenate training and test\n",
    "raw_data=pd.concat([train.loc[:,fts[0]:fts[-1]], test.loc[:, fts[0]:fts[-1]]])\n",
    "print('shape of train plus test', raw_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at Nas in important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements in feature_na 35\n",
      "number of features in feature_importance 208\n",
      "number of feature_intersect 25\n"
     ]
    }
   ],
   "source": [
    "raw_data_nas=raw_data.isnull().sum()\n",
    "raw_data_nas=raw_data_nas.sort_values(ascending=False)\n",
    "raw_data_nas=pd.DataFrame(raw_data_nas, columns=['na_counts'])\n",
    "raw_data_nas=raw_data_nas[raw_data_nas.na_counts>0.1*(n_train+n_test)]\n",
    "\n",
    "feature_na=set(raw_data_nas.index)\n",
    "print('number of elements in feature_na', len(feature_na))\n",
    "\n",
    "feature_important=pd.read_csv('features_importance_rate_0.01_withallmacro')\n",
    "feature_important=set(feature_important.feature.values)\n",
    "print('number of features in feature_importance', len(feature_important))\n",
    "\n",
    "feature_intersect=feature_important & feature_na\n",
    "print('number of feature_intersect', len(feature_intersect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinlin/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/xinlin/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/xinlin/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/xinlin/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/xinlin/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/xinlin/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/xinlin/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/xinlin/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "raw_data[list(feature_intersect)].iloc[1:20,:]\n",
    "\n",
    "#if build year if larger than 2019 or smaller than 1800 change them to median\n",
    "raw_data['build_year'][(raw_data.build_year<1800) | (raw_data.build_year>2019)]=raw_data.build_year.median()\n",
    "\n",
    "#if full sq is zero, assign it to median\n",
    "raw_data['full_sq'][raw_data.full_sq==0]=raw_data.full_sq.median()\n",
    "\n",
    "#if max_floor is zero, assign it to median\n",
    "raw_data['max_floor'][raw_data.max_floor==0]=raw_data.full_sq.median()\n",
    "\n",
    "#preschool quota is zero assign it to median\n",
    "raw_data['preschool_quota'][raw_data.preschool_quota==0]=raw_data.preschool_quota.median()\n",
    "\n",
    "#if kitch_sq is too large or larger than life_sq use 20% full_sq\n",
    "raw_data['kitch_sq'][(raw_data.kitch_sq > 50) | (raw_data.kitch_sq > raw_data.life_sq)]=kitch_est=raw_data['full_sq'][(raw_data.kitch_sq > 50) | (raw_data.kitch_sq > raw_data.life_sq)]*0.2\n",
    "\n",
    "#if life_sq is na or life_sq is too small or too large, change it to 0.7 full_sq\n",
    "raw_data['life_sq'][raw_data.life_sq.isnull()]=raw_data['full_sq'][raw_data.life_sq.isnull()]*0.7\n",
    "raw_data['life_sq'][(raw_data.life_sq<0.1*raw_data.full_sq)| (raw_data.life_sq>raw_data.full_sq)] = raw_data['full_sq'][(raw_data.life_sq<0.1*raw_data.full_sq)| (raw_data.life_sq>raw_data.full_sq)]\n",
    "\n",
    "#fill no data with satisfactory\n",
    "raw_data['ecology'][raw_data.ecology=='no data']='satisfactory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add month-year count\n",
    "month_year = (raw_data.timestamp.dt.month + raw_data.timestamp.dt.year*100)\n",
    "month_year_cnt_map=month_year.value_counts().to_dict()\n",
    "raw_data['month_year_cnt']=month_year.map(month_year_cnt_map)\n",
    "\n",
    "#Add week-year count\n",
    "week_year=(raw_data.timestamp.dt.week + raw_data.timestamp.dt.year*100)\n",
    "week_year_cnt_map=week_year.value_counts().to_dict()\n",
    "raw_data['week_year_cnt']=week_year.map(week_year_cnt_map)\n",
    "\n",
    "#Add month and day-of-week\n",
    "raw_data['month']=raw_data.timestamp.dt.month\n",
    "raw_data['dow']=raw_data.timestamp.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_f=[\"timestamp\", \"balance_trade_growth\", \"eurrub\", \"average_provision_of_build_contract\", \n",
    "\"micex_rgbi_tr\", \"micex_cbi_tr\", \"deposits_rate\", \"mortgage_value\", \n",
    "\"mortgage_rate\", \"income_per_cap\", \"rent_price_4+room_bus\",\"museum_visitis_per_100_cap\",\"apartment_build\"]\n",
    "\n",
    "selected_macro=macro[selected_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the merged data (36697, 306)\n"
     ]
    }
   ],
   "source": [
    "raw_data=pd.merge(raw_data, selected_macro, how='left', on='timestamp')\n",
    "print('shape of the merged data', raw_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after get_dummies (36697, 466)\n"
     ]
    }
   ],
   "source": [
    "data=pd.get_dummies(raw_data)\n",
    "print('shape after get_dummies', data.shape)\n",
    "\n",
    "#change timestamp to year\n",
    "data['year']=data['timestamp'].dt.year.astype(int)\n",
    "data.drop('timestamp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill NaN with median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of NaN in train and test 333386\n",
      "number of NaN in train and test 0\n"
     ]
    }
   ],
   "source": [
    "#when using get_dummies the Nan in categorical data are ignored. The possible Nan are numbers. fill them with mean\n",
    "print('number of NaN in train and test', data.isnull().sum().sum())\n",
    "\n",
    "data=data.fillna(data.median())\n",
    "\n",
    "print('number of NaN in train and test', data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add more features:\n",
    "\n",
    "#relative floor\n",
    "data['relative_floor']=data['floor'].div(data['max_floor'].astype(float))\n",
    "\n",
    "#relative life_sq\n",
    "data['relative_life_sq']=data['life_sq'].div(data['full_sq'].astype(float))\n",
    "\n",
    "#ratio of number of pupils and preschool seats\n",
    "data['ratio_preschool']=data['children_preschool'].div(data['preschool_quota'].astype(float))\n",
    "\n",
    "#ratio of number of pupils and school seats\n",
    "data['ratio_school']=data['children_school'].div(data['school_quota'].astype(float))\n",
    "\n",
    "#young porpulation ratio\n",
    "data['ratio_young']=data['young_all'].div(data['full_all'].astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Try standardscaler before fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "col_names=list(data.columns)\n",
    "\n",
    "ss=StandardScaler(with_mean=False, with_std=True)\n",
    "data_std=ss.fit_transform(data)\n",
    "data_std=pd.DataFrame(data_std, columns=col_names)\n",
    "\n",
    "train_std=data_std.iloc[:n_train, :]\n",
    "test_std=data_std.iloc[n_train :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 26.9min\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed: 27.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=10,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=800, n_jobs=-1, oob_score=False, random_state=None,\n",
       "           verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train rf:\n",
    "\n",
    "rf=RandomForestRegressor(n_estimators=800, n_jobs=-1, \n",
    "                         max_features='auto', max_depth=10, verbose=1)\n",
    "rf.fit(train_std, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rf_index 14\n"
     ]
    }
   ],
   "source": [
    "rf_importance=rf.feature_importances_\n",
    "rf_dict=dict()\n",
    "\n",
    "for f, importance in zip (train_std.columns, rf_importance):\n",
    "    rf_dict[f]=importance\n",
    "    \n",
    "rf_dict=sorted(rf_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "rf_dict=pd.DataFrame(rf_dict, columns=['feature', 'score'])\n",
    "#rf_dict.to_csv(path_or_buf='submissions/0527/rf_feature_importance.csv')\n",
    "\n",
    "importance_rate=0.01\n",
    "rf_index=rf_dict[rf_dict.score>importance_rate*rf_dict.score.max()].feature.values\n",
    "print('number of rf_index', len(rf_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   40.4s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=10,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=800, n_jobs=-1, oob_score=False, random_state=None,\n",
       "           verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_new=RandomForestRegressor(n_estimators=800, n_jobs=-1,\n",
    "                            max_features='auto', max_depth=10, verbose=1)\n",
    "rf_new.fit(train_std[rf_index], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 800 out of 800 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30474</td>\n",
       "      <td>5101030.540339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30475</td>\n",
       "      <td>8362429.300093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30476</td>\n",
       "      <td>6274865.593472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30477</td>\n",
       "      <td>6374278.885423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30478</td>\n",
       "      <td>5145806.001251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       price_doc\n",
       "0  30474  5101030.540339\n",
       "1  30475  8362429.300093\n",
       "2  30476  6274865.593472\n",
       "3  30477  6374278.885423\n",
       "4  30478  5145806.001251"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_rf=rf_new.predict(train_std[rf_index])\n",
    "in_rf=np.exp(in_rf)-1\n",
    "insample_rf=pd.DataFrame({'id': train.id, 'price_doc_rf': in_rf})\n",
    "insample_rf.to_csv(path_or_buf='ensamble/rf_train_0.01importance.csv', index=False)\n",
    "\n",
    "pre_rf=rf_new.predict(test_std[rf_index])\n",
    "pre_rf=np.exp(pre_rf)-1\n",
    "\n",
    "submission=pd.DataFrame({'id': test_id, 'price_doc': pre_rf})\n",
    "submission.to_csv(path_or_buf='ensamble/170527subission_rf_with_12macro_cleandata_8newfeature.cs_0.01importv',index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinlin/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_part (23228, 471)\n",
      "shape of target_part (23228,)\n",
      "shape of val_train (5807, 471)\n",
      "shape of val_target (5807,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_part, val_train, target_part, val_target=train_test_split(train_std, target, test_size=0.2, \n",
    "                                                              random_state=random_state)\n",
    "print('shape of train_part', train_part.shape)\n",
    "print('shape of target_part', target_part.shape)\n",
    "print('shape of val_train', val_train.shape)\n",
    "print('shape of val_target', val_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=train_part.columns\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-rmse:8.52292\n",
      "Will train until validation-rmse hasn't improved in 20 rounds.\n",
      "[100]\tvalidation-rmse:1.22996\n",
      "[200]\tvalidation-rmse:0.388079\n",
      "[300]\tvalidation-rmse:0.332577\n",
      "[400]\tvalidation-rmse:0.326317\n",
      "[500]\tvalidation-rmse:0.323773\n",
      "[600]\tvalidation-rmse:0.32226\n",
      "[700]\tvalidation-rmse:0.32147\n",
      "[800]\tvalidation-rmse:0.321004\n",
      "[900]\tvalidation-rmse:0.320729\n",
      "[1000]\tvalidation-rmse:0.320453\n",
      "Stopping. Best iteration:\n",
      "[988]\tvalidation-rmse:0.320446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtrain=xgb.DMatrix(train_part, target_part)\n",
    "dval=xgb.DMatrix(val_train, val_target)\n",
    "dtest=xgb.DMatrix(test_std)\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.02,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 0,\n",
    "    'lambda': 100,\n",
    "    'base_score': 7\n",
    "}\n",
    "\n",
    "model=xgb.train(xgb_params, dtrain, num_boost_round=2000, \n",
    "                evals=[(dval, 'validation')], early_stopping_rounds=20,\n",
    "               verbose_eval=100)\n",
    "num_boost_round=model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of f_index 133\n"
     ]
    }
   ],
   "source": [
    "score=model.get_fscore()\n",
    "score=sorted(score.items(), key=operator.itemgetter(1), reverse=True)\n",
    "score=pd.DataFrame(score, columns=['feature', 'fscore'])\n",
    "score.to_csv(path_or_buf='submissions/scores/xgb_score.csv')\n",
    "\n",
    "importance_rate=0.03\n",
    "f_index=score[score.fscore>importance_rate*score.fscore.max()].feature.values\n",
    "print('number of f_index', len(f_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-rmse:8.52292\n",
      "Will train until validation-rmse hasn't improved in 20 rounds.\n",
      "[100]\tvalidation-rmse:1.23039\n",
      "[200]\tvalidation-rmse:0.389826\n",
      "[300]\tvalidation-rmse:0.333597\n",
      "[400]\tvalidation-rmse:0.326868\n",
      "[500]\tvalidation-rmse:0.324236\n",
      "[600]\tvalidation-rmse:0.322624\n",
      "[700]\tvalidation-rmse:0.321829\n",
      "[800]\tvalidation-rmse:0.32117\n",
      "[900]\tvalidation-rmse:0.320711\n",
      "Stopping. Best iteration:\n",
      "[932]\tvalidation-rmse:0.320608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_select=pd.DataFrame(train_part)\n",
    "train_select=train_select[f_index]\n",
    "\n",
    "val_select=pd.DataFrame(val_train)\n",
    "val_select=val_select[f_index]\n",
    "\n",
    "dtrain_select=xgb.DMatrix(train_select, target_part)\n",
    "dval_select=xgb.DMatrix(val_select, val_target)\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.02,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 0,\n",
    "    'lambda': 100,\n",
    "    'base_score': 7\n",
    "}\n",
    "\n",
    "model=xgb.train(xgb_params, dtrain_select, num_boost_round=2000, \n",
    "                evals=[(dval_select, 'validation')], early_stopping_rounds=20,\n",
    "               verbose_eval=100)\n",
    "num_boost_round=model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train xgb with full train data\n",
    "xgb_params = {\n",
    "    'eta': 0.02,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 0,\n",
    "    'lambda': 100,\n",
    "    'base_score': 7\n",
    "}\n",
    "\n",
    "Ddata_std=xgb.DMatrix(train_std[f_index], target)\n",
    "#Ddata_std=xgb.DMatrix(train_std, target)\n",
    "full_model=xgb.train(xgb_params, Ddata_std, num_boost_round=num_boost_round,\n",
    "                    verbose_eval=100)\n",
    "#xgb.cv(xgb_params, Ddata_std, num_boost_round=3000, \n",
    "#       nfold=5, verbose_eval=100, early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_doc_xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30474</td>\n",
       "      <td>5619100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30475</td>\n",
       "      <td>8844294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30476</td>\n",
       "      <td>5284690.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30477</td>\n",
       "      <td>6106908.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30478</td>\n",
       "      <td>5312507.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  price_doc_xgb\n",
       "0  30474      5619100.0\n",
       "1  30475      8844294.0\n",
       "2  30476      5284690.0\n",
       "3  30477      6106908.0\n",
       "4  30478      5312507.5"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_xgb=full_model.predict(Ddata_std)\n",
    "in_xgb=np.exp(in_xgb)-1\n",
    "insample_xgb=pd.DataFrame({'id': train.id, 'price_doc_xgb': in_xgb})\n",
    "insample_xgb.to_csv(path_or_buf='ensamble/xgb_train_0.03importance.csv', index=False)\n",
    "\n",
    "dtest_std=xgb.DMatrix(test_std[f_index])\n",
    "pre_xgb=full_model.predict(dtest_std)\n",
    "pre_xgb=np.exp(pre_xgb)-1\n",
    "\n",
    "submission=pd.DataFrame({'id': test_id, 'price_doc_xgb': pre_xgb})\n",
    "submission.to_csv(path_or_buf='ensamble/170527subission_xgb_with_12macro_cleandata_8newfeature_0.03import.csv',index=False)\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30474</td>\n",
       "      <td>5860943.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30475</td>\n",
       "      <td>8614750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30476</td>\n",
       "      <td>4888695.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30477</td>\n",
       "      <td>6142954.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30478</td>\n",
       "      <td>5388001.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  price_doc\n",
       "0  30474  5860943.5\n",
       "1  30475  8614750.0\n",
       "2  30476  4888695.5\n",
       "3  30477  6142954.0\n",
       "4  30478  5388001.5"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_xgb=np.exp(pre_xgb)-1\n",
    "\n",
    "submission=pd.DataFrame({'id': test_id, 'price_doc': pre_xgb})\n",
    "submission.to_csv(path_or_buf='170525subission_xgb_with_12macro_cleandata_8newfeature_nosubsample_0.03impoartance.csv',index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GBRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "GBRT_clf=GradientBoostingRegressor(n_estimators=300,learning_rate=0.03,\n",
    "                                   verbose=1, random_state=random_state)\n",
    "\n",
    "#clf.fit(train_std[f_index], target)\n",
    "#clf.fit(train_std, target)\n",
    "\n",
    "GBRT_params={'loss': ['ls', 'lad'],\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "            'max_leaf_nodes': [7,8,9,10]}\n",
    "\n",
    "GBRT_cv=GridSearchCV(GBRT_clf, GBRT_params, cv=5, n_jobs=-1)\n",
    "GBRT_cv.fit(train_std, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.02, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=10, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=600,\n",
       "             presort='auto', random_state=7, subsample=0.9, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBRT_clf=GradientBoostingRegressor(n_estimators=600,learning_rate=0.02,\n",
    "                                   verbose=0, random_state=random_state,\n",
    "                                  subsample=0.9, max_leaf_nodes=10)\n",
    "\n",
    "GBRT_clf.fit(train_std, target)\n",
    "\n",
    "# pre_GBRT=GBRT_clf.predict(test_std)\n",
    "# pre_GBRT=np.exp(pre_GBRT)-1\n",
    "\n",
    "# submission=pd.DataFrame({'id': test_id, 'price_doc': pre_GBRT})\n",
    "# submission.to_csv(path_or_buf='170526submission_GBRT_with_12features_cleandata_macro.csv',index=False)\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of f_index 135\n"
     ]
    }
   ],
   "source": [
    "GBRT_dict=dict()\n",
    "for f, importance in zip(train_std.columns, GBRT_clf.feature_importances_):\n",
    "    GBRT_dict[f]=importance\n",
    "\n",
    "GBRT_dict=sorted(GBRT_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "GBRT_dict=pd.DataFrame(GBRT_dict, columns=['feature', 'GBRT_score'])\n",
    "GBRT_dict.to_csv(path_or_buf='submissions/scores/GBRT_score.csv')\n",
    "\n",
    "importance_rate=0.01\n",
    "GBRT_index=GBRT_dict[xgb_dict.GBRT_score>importance_rate*GBRT_dict.GBRT_score.max()].feature.values\n",
    "print('number of f_index', len(GBRT_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.02, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=10, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=600,\n",
       "             presort='auto', random_state=7, subsample=0.9, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBRT_fullmode=GradientBoostingRegressor(n_estimators=600,learning_rate=0.02,\n",
    "                                   verbose=0, random_state=random_state,\n",
    "                                  subsample=0.9, max_leaf_nodes=10)\n",
    "\n",
    "#scores = cross_val_score(GBRT_cv, train_std[GBRT_index], target, cv=5)\n",
    "#print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std()))\n",
    "GBRT_fullmode.fit(train_std[GBRT_index], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_doc_GBRT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30474</td>\n",
       "      <td>5633902.113439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30475</td>\n",
       "      <td>8287396.831267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30476</td>\n",
       "      <td>5465193.683202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30477</td>\n",
       "      <td>5576789.089920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30478</td>\n",
       "      <td>5103606.320849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  price_doc_GBRT\n",
       "0  30474  5633902.113439\n",
       "1  30475  8287396.831267\n",
       "2  30476  5465193.683202\n",
       "3  30477  5576789.089920\n",
       "4  30478  5103606.320849"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_GBRT=GBRT_fullmode.predict(train_std[GBRT_index])\n",
    "in_GBRT=np.exp(in_GBRT)-1\n",
    "insample_GBRT=pd.DataFrame({'id': train.id, 'price_doc_GBRT': in_GBRT})\n",
    "insample_GBRT.to_csv(path_or_buf='ensamble/GBRT_train_0.03importance.csv', index=False)\n",
    "\n",
    "\n",
    "pre_GBRT=GBRT_fullmode.predict(test_std[GBRT_index])\n",
    "pre_GBRT=np.exp(pre_GBRT)-1\n",
    "\n",
    "submission=pd.DataFrame({'id': test_id, 'price_doc_GBRT': pre_GBRT})\n",
    "submission.to_csv(path_or_buf='ensamble/170527submission_GBRT_with_12features_macro_0.03import.csv',index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
